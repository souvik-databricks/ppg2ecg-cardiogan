{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsoBs6OzhFui",
    "outputId": "917bf551-d961-428d-b289-ade6d1d97b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzONeQ72hY1Z",
    "outputId": "a2fbd19c-4c2c-4f9f-e271-17d37a2da6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-22 09:53:50--  https://github.com/pritamqu/ppg2ecg-cardiogan/releases/download/model_weights/cardiogan_ppg2ecg_generator.zip\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/380089430/7516ac44-7196-4784-9d01-3766ca939ae6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230622%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230622T095350Z&X-Amz-Expires=300&X-Amz-Signature=ae518c173a31f0510c252a265083a9a87b8551db25d1b85afae8664d4bec7cbe&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=380089430&response-content-disposition=attachment%3B%20filename%3Dcardiogan_ppg2ecg_generator.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-06-22 09:53:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/380089430/7516ac44-7196-4784-9d01-3766ca939ae6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230622%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230622T095350Z&X-Amz-Expires=300&X-Amz-Signature=ae518c173a31f0510c252a265083a9a87b8551db25d1b85afae8664d4bec7cbe&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=380089430&response-content-disposition=attachment%3B%20filename%3Dcardiogan_ppg2ecg_generator.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 216383914 (206M) [application/octet-stream]\n",
      "Saving to: ‘cardiogan_ppg2ecg_generator.zip’\n",
      "\n",
      "cardiogan_ppg2ecg_g 100%[===================>] 206.36M  63.4MB/s    in 3.3s    \n",
      "\n",
      "2023-06-22 09:53:53 (63.4 MB/s) - ‘cardiogan_ppg2ecg_generator.zip’ saved [216383914/216383914]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/pritamqu/ppg2ecg-cardiogan/releases/download/model_weights/cardiogan_ppg2ecg_generator.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVF485DOhhj9",
    "outputId": "c2719412-97b6-40cb-c955-2172a587eb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cardiogan_ppg2ecg_generator.zip\n",
      "   creating: weights/\n",
      "  inflating: weights/checkpoint      \n",
      "  inflating: weights/ckpt-1.data-00000-of-00001  \n",
      "  inflating: weights/ckpt-1.index    \n"
     ]
    }
   ],
   "source": [
    "!unzip cardiogan_ppg2ecg_generator.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP4k4taQhqFx",
    "outputId": "fcb88126-28e3-468d-d805-adf02b89889a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.12.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, jax, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: dopamine-rl\n"
     ]
    }
   ],
   "source": [
    "# pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcEd-Tt7h3wu",
    "outputId": "1e798be8-028f-4e41-8c32-6b22649cfa2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow-addons\n",
      "  Using cached tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
      "Collecting biosppy\n",
      "  Using cached biosppy-1.0.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting tk\n",
      "  Downloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting bidict (from biosppy)\n",
      "  Downloading bidict-0.22.1-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.8.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.10.1)\n",
      "Collecting shortuuid (from biosppy)\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.2.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from biosppy) (4.7.0.72)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->biosppy) (3.1.0)\n",
      "Installing collected packages: tk, typeguard, shortuuid, bidict, tensorflow-addons, biosppy\n",
      "Successfully installed bidict-0.22.1 biosppy-1.0.0 shortuuid-1.0.11 tensorflow-addons-0.20.0 tk-0.1.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons biosppy tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd7bQItbizlS",
    "outputId": "f3d28d4e-f1d7-4ec9-eaf6-83ff589a29dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from biosppy.signals import tools as tools\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "import cv2\n",
    "import sklearn.preprocessing as skp\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW3Rh_NNjm6b"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QbhgQl7YjZRy"
   },
   "outputs": [],
   "source": [
    "def filter_ecg(signal, sampling_rate):\n",
    "\n",
    "    signal = np.array(signal)\n",
    "    order = int(0.3 * sampling_rate)\n",
    "    filtered, _, _ = tools.filter_signal(signal=signal,\n",
    "                                  ftype='FIR',\n",
    "                                  band='bandpass',\n",
    "                                  order=order,\n",
    "                                  frequency=[3, 45],\n",
    "                                  sampling_rate=sampling_rate)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def filter_ppg(signal, sampling_rate):\n",
    "\n",
    "    signal = np.array(signal)\n",
    "    sampling_rate = float(sampling_rate)\n",
    "    filtered, _, _ = tools.filter_signal(signal=signal,\n",
    "                                  ftype='butter',\n",
    "                                  band='bandpass',\n",
    "                                  order=4, #3\n",
    "                                  frequency=[1, 8], #[0.5, 8]\n",
    "                                  sampling_rate=sampling_rate)\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYh0RCNtjs23"
   },
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iLJfnFaUjxgJ"
   },
   "outputs": [],
   "source": [
    "weights_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02, seed=None)\n",
    "\n",
    "def Dense(units, activation=None):\n",
    "    op = tf.keras.layers.Dense(units=units, activation=activation, use_bias=True, kernel_initializer=weights_initializer,\n",
    "                               bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                               kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "    return op\n",
    "\n",
    "def Conv1D(filters, kernel_size, strides=1, padding='valid', activation=None, use_bias=True):\n",
    "    op = tf.keras.layers.Conv2D(filters=filters, kernel_size=(1, kernel_size), strides=(1, strides), padding=padding, data_format='channels_last',\n",
    "                                dilation_rate=1, activation=None, use_bias=use_bias,\n",
    "                                kernel_initializer=weights_initializer, bias_initializer='zeros',\n",
    "                                kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                                kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "    return op\n",
    "\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "\n",
    "def DeConv1D(filters, kernel_size, strides=1, padding='valid', use_bias=True):\n",
    "    op = tf.keras.layers.Conv2DTranspose(filters= filters, kernel_size=(1, kernel_size), strides=(1, strides), padding=padding,\n",
    "                                         output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=use_bias,\n",
    "                                         kernel_initializer=weights_initializer, bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                         bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "    return op\n",
    "\n",
    "def BatchNormalization(trainable=True, virtual_batch_size=None):\n",
    "    op = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n",
    "                                            beta_initializer='zeros', gamma_initializer='ones',\n",
    "                                            moving_mean_initializer='zeros', moving_variance_initializer='ones',\n",
    "                                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,\n",
    "                                            gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99,\n",
    "                                            fused=None, trainable=trainable, virtual_batch_size=virtual_batch_size, adjustment=None, name=None)\n",
    "\n",
    "    return op\n",
    "\n",
    "def Activation(x, activation):\n",
    "\n",
    "    if activation == 'relu':\n",
    "        return tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
    "    elif activation == 'leaky_relu':\n",
    "        return tf.keras.activations.relu(x, alpha=0.2, max_value=None, threshold=0)\n",
    "    elif activation == 'sigmoid':\n",
    "        return tf.keras.activations.sigmoid(x)\n",
    "    elif activation == 'softmax':\n",
    "        return tf.keras.activations.softmax(x, axis=-1)\n",
    "    elif activation == 'tanh':\n",
    "        return tf.keras.activations.tanh(x)\n",
    "    else:\n",
    "        raise ValueError('please check the name of the activation')\n",
    "\n",
    "\n",
    "def Dropout(rate):\n",
    "    op = tf.keras.layers.Dropout(rate=rate, noise_shape=None, seed=None)\n",
    "\n",
    "    return op\n",
    "\n",
    "def flatten():\n",
    "    op = tf.keras.layers.Flatten(data_format=None)\n",
    "\n",
    "    return op\n",
    "\n",
    "def normalization(name):\n",
    "\n",
    "    if name =='none':\n",
    "        return lambda: lambda x: x\n",
    "    elif name == 'batch_norm':\n",
    "        return keras.layers.BatchNormalization()\n",
    "    elif name == 'instance_norm':\n",
    "        return tfa.layers.InstanceNormalization()\n",
    "    elif name == 'layer_norm':\n",
    "        return keras.layers.LayerNormalization()\n",
    "\n",
    "def attention_block_1d(curr_layer, conn_layer):\n",
    "\n",
    "    \"\"\" adopted from https://github.com/lixiaolei1982/Keras-Implementation-of-U-Net-R2U-Net-Attention-U-Net-Attention-R2U-Net.-/blob/master/network.py\n",
    "    \"\"\"\n",
    "    # theta_x(?,g_height,g_width,inter_channel)\n",
    "\n",
    "    inter_channel = curr_layer.get_shape().as_list()[3] #//4\n",
    "\n",
    "    theta_x = Conv1D(inter_channel, 1, 1)(conn_layer)\n",
    "\n",
    "    # phi_g(?,g_height,g_width,inter_channel)\n",
    "\n",
    "    phi_g = Conv1D(inter_channel, 1, 1)(curr_layer)\n",
    "\n",
    "    # f(?,g_height,g_width,inter_channel)\n",
    "\n",
    "    f = Activation(keras.layers.add([theta_x, phi_g]), 'relu')\n",
    "\n",
    "    # psi_f(?,g_height,g_width,1)\n",
    "\n",
    "    psi_f = Conv1D(1, 1, 1)(f)\n",
    "\n",
    "    rate = Activation(psi_f, 'sigmoid')\n",
    "\n",
    "    # rate(?,x_height,x_width)\n",
    "\n",
    "    # att_x(?,x_height,x_width,x_channel)\n",
    "\n",
    "    att_x = keras.layers.multiply([conn_layer, rate])\n",
    "\n",
    "    return att_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Iw_8RKj1vw"
   },
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CGFWzSWqj9V3"
   },
   "outputs": [],
   "source": [
    "def generator_attention(input_shape=512,\n",
    "                      filter_size=[64, 128, 256, 512, 512, 512],\n",
    "                      kernel_size=[16, 16, 16, 16, 16, 16],\n",
    "                      n_downsample=6,\n",
    "                      norm='layer_norm',\n",
    "                      skip_connection=True):\n",
    "\n",
    "    \"\"\"\n",
    "    input_shape = 128*4\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def _downsample(ip, filter_size, kernel_size, norm, stride_size=2):\n",
    "\n",
    "        ip = Conv1D(filters=filter_size, kernel_size=kernel_size, strides=stride_size, padding='same', use_bias=False)(ip)\n",
    "        # ip = tf.dtypes.cast(ip, tf.float32)\n",
    "        if norm != 'none':\n",
    "            ip = normalization(norm)(ip)\n",
    "        ip = Activation(ip, activation='leaky_relu')\n",
    "\n",
    "        return ip\n",
    "\n",
    "    def _upsample(ip, filter_size, kernel_size, norm, stride_size=2, drop_rate = 0.5, apply_dropout=False):\n",
    "\n",
    "        ip = DeConv1D(filters=filter_size, kernel_size=kernel_size, strides=stride_size, padding='same', use_bias=False)(ip)\n",
    "        # ip = tf.dtypes.cast(ip, tf.float32)\n",
    "        if norm != 'none':\n",
    "            ip = normalization(norm)(ip)\n",
    "        if apply_dropout:\n",
    "            ip = Dropout(rate=drop_rate)\n",
    "        ip = Activation(ip, activation='relu')\n",
    "\n",
    "        return ip\n",
    "\n",
    "\n",
    "    ## input\n",
    "    h = inputs = keras.Input(shape=input_shape) # None, 512\n",
    "    h = tf.expand_dims(h, axis=1) # None, 1, 512\n",
    "    h = tf.expand_dims(h, axis=3) # None, 1, 512, 1\n",
    "\n",
    "    ## downsample\n",
    "    connections = []\n",
    "    for k in range(n_downsample):\n",
    "\n",
    "        # filter_size *=2\n",
    "        # kernel_size = kernel_size\n",
    "        if k==0:\n",
    "            h =  _downsample(h, filter_size[k], kernel_size[k], 'none')\n",
    "        else:\n",
    "            h =  _downsample(h, filter_size[k], kernel_size[k], norm)\n",
    "\n",
    "        connections.append(h)\n",
    "\n",
    "    ## upsampling`\n",
    "    # filter_size = filter_size//2\n",
    "    h = _upsample(h, filter_size[k], kernel_size[k], norm, stride_size=1)\n",
    "    if skip_connection:\n",
    "        _h = attention_block_1d(curr_layer= h, conn_layer= connections[n_downsample-1])\n",
    "        h  = keras.layers.add([h, _h])\n",
    "\n",
    "\n",
    "    for l in range(1, n_downsample):\n",
    "\n",
    "        h  = _upsample(h, filter_size[k-l], kernel_size[k-l], norm)\n",
    "        if skip_connection:\n",
    "            _h = attention_block_1d(curr_layer= h, conn_layer= connections[k-l])\n",
    "            h  = keras.layers.add([h, _h])\n",
    "\n",
    "    ## output\n",
    "    h = DeConv1D(filters=1, kernel_size=kernel_size[k-l], strides=2, padding='same')(h)\n",
    "    h = Activation(h, activation='tanh')\n",
    "    h = tf.squeeze(h, axis=1)\n",
    "    h = tf.squeeze(h, axis=2)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nmYrP51kBxh"
   },
   "source": [
    "## Enhanced Checkpointing for tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "--oRQPKokFIa"
   },
   "outputs": [],
   "source": [
    "class Checkpoint:\n",
    "    \"\"\"Enhanced \"tf.train.Checkpoint\".\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 checkpoint_kwargs,  # for \"tf.train.Checkpoint\"\n",
    "                 directory,  # for \"tf.train.CheckpointManager\"\n",
    "                 max_to_keep=5,\n",
    "                 keep_checkpoint_every_n_hours=None):\n",
    "        self.checkpoint = tf.train.Checkpoint(**checkpoint_kwargs)\n",
    "        self.manager = tf.train.CheckpointManager(self.checkpoint, directory, max_to_keep, keep_checkpoint_every_n_hours)\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        save_path = self.manager.latest_checkpoint if save_path is None else save_path\n",
    "        return self.checkpoint.restore(save_path)\n",
    "\n",
    "    def save(self, file_prefix_or_checkpoint_number=None, session=None):\n",
    "        if isinstance(file_prefix_or_checkpoint_number, str):\n",
    "            return self.checkpoint.save(file_prefix_or_checkpoint_number, session=session)\n",
    "        else:\n",
    "            return self.manager.save(checkpoint_number=file_prefix_or_checkpoint_number)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if hasattr(self.checkpoint, attr):\n",
    "            return getattr(self.checkpoint, attr)\n",
    "        elif hasattr(self.manager, attr):\n",
    "            return getattr(self.manager, attr)\n",
    "        else:\n",
    "            self.__getattribute__(attr)  # this will raise an exception\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTZx3ffGke5a"
   },
   "source": [
    "## Running CardioGAN CPU (lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "8SvCMLsDkcEK",
    "outputId": "aab8d6b0-8bcf-44ee-b5ee-a8cbd2a544d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).ep_cnt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' please process the data as mentioned below before extracting ECG output '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def sample_P2E(P, model):\n",
    "    fake_ecg = model(P, training=False)\n",
    "    return fake_ecg\n",
    "\n",
    "########### params ###########\n",
    "ecg_sampling_freq = 128\n",
    "ppg_sampling_freq = 128\n",
    "window_size = 4\n",
    "ecg_segment_size = ecg_sampling_freq*window_size\n",
    "ppg_segment_size = ppg_sampling_freq*window_size\n",
    "model_dir = '/content/weights'\n",
    "\n",
    "\"\"\" model \"\"\"\n",
    "Gen_PPG2ECG = generator_attention()\n",
    "\"\"\" restore \"\"\"\n",
    "Checkpoint(dict(Gen_PPG2ECG=Gen_PPG2ECG), model_dir).restore()\n",
    "# tflib.Checkpoint(dict(Gen_PPG2ECG=Gen_PPG2ECG), model_dir).restore()\n",
    "print(\"model loaded successfully\")\n",
    "\n",
    "\"\"\" please process the data as mentioned below before extracting ECG output \"\"\"\n",
    "# load the data:\n",
    "# x_ppg = np.loadtxt()\n",
    "# make sure loaded data is a numpy array:\n",
    "# x_ppg = np.array(x_ppg)\n",
    "# resample to 128 Hz using:\n",
    "# cv2.resize(x_ppg, (1,ppg_segment_size), interpolation = cv2.INTER_LINEAR)\n",
    "# filter the data using:\n",
    "# filter_ppg(x_ppg, 128)\n",
    "# make an array to N x 512 [this is the input shape of x_ppg], where Nx512=len(x_ppg)\n",
    "# normalize the data b/w -1 to 1:\n",
    "# x_ppg = skp.minmax_scale(x_ppg, (-1, 1), axis=1)\n",
    "#######\n",
    "# x_ecg = sample_P2E(x_ppg, Gen_PPG2ECG)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAWt3AUgk8kf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
